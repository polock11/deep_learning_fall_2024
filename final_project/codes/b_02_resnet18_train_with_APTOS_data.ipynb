{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with APTOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The augmentation techniques applied here include random cropping, which extracts a random region from the image to introduce spatial variation, and random padding to ensure consistent image size while shifting the content spatially. The pipeline also includes random rotations (up to ±30 degrees) and random horizontal and vertical flips to make the model robust to different orientations. Additionally, color jittering is applied to simulate variations in lighting conditions by adjusting brightness. These augmentations enhance the diversity of the training data, improving the model's ability to generalize to unseen variations in real-world images.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLORandomPad:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        pad_width = max(0, self.size[0] - img.width)\n",
    "        pad_height = max(0, self.size[1] - img.height)\n",
    "        pad_left = random.randint(0, pad_width)\n",
    "        pad_top = random.randint(0, pad_height)\n",
    "        pad_right = pad_width - pad_left\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n",
    "\n",
    "\n",
    "class FundRandomRotate:\n",
    "    def __init__(self, prob, degree):\n",
    "        self.prob = prob\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.prob:\n",
    "            angle = random.uniform(-self.degree, self.degree)\n",
    "            return transforms.functional.rotate(img, angle)\n",
    "        return img\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop((210, 210)),\n",
    "    SLORandomPad((224, 224)),\n",
    "    FundRandomRotate(prob=0.5, degree=30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, ann_file, image_dir, transform=None, mode='single', test=False):\n",
    "        self.ann_file = ann_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.test = test\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'single':\n",
    "            self.data = self.load_data()\n",
    "        else:\n",
    "            self.data = self.load_data_dual()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'single':\n",
    "            return self.get_item(index)\n",
    "        else:\n",
    "            return self.get_item_dual(index)\n",
    "\n",
    "    # 1. single image\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_info = dict()\n",
    "            file_info['id_code'] = os.path.join(self.image_dir, row['id_code'])\n",
    "            if not self.test:\n",
    "                file_info['diagnosis'] = int(row['diagnosis'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item(self, index):\n",
    "        data = self.data[index]\n",
    "        img = Image.open(data['id_code']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if not self.test:\n",
    "            label = torch.tensor(data['diagnosis'], dtype=torch.int64)\n",
    "            return img, label\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = models.resnet18(pretrained=True, )\n",
    "        self.backbone.fc = nn.Identity()  # Remove the original classification layer\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with self-attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Backbone (ResNet18)\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the original classification layer\n",
    "\n",
    "        # Self-Attention Layer\n",
    "        self.attention = SelfAttention(512)  # 512 is the feature size from ResNet18\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet18 backbone\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "\n",
    "        # Compute Query, Key, and Value matrices\n",
    "        Q = self.query(x)  # [batch_size, input_dim]\n",
    "        K = self.key(x)    # [batch_size, input_dim]\n",
    "        V = self.value(x)  # [batch_size, input_dim]\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)  # [batch_size, batch_size]\n",
    "        attention_weights = self.softmax(attention_scores)  # [batch_size, batch_size]\n",
    "\n",
    "        # Compute the attention output\n",
    "        attended_features = torch.matmul(attention_weights, V)  # [batch_size, input_dim]\n",
    "\n",
    "        return attended_features\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_image_ids = []\n",
    "\n",
    "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            if test_only:\n",
    "                images = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "\n",
    "            \n",
    "            \n",
    "            if not isinstance(images, list):\n",
    "                images = images.to(device)  # single image case\n",
    "            else:\n",
    "                images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                # single image case\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "                \"\"\"\n",
    "                image_ids = [\n",
    "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
    "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
    "                ]\n",
    "                all_image_ids.extend(image_ids)\n",
    "                if not test_only:\n",
    "                    all_labels.extend(labels.numpy())\n",
    "            else:\n",
    "                # dual images case\n",
    "                for k in range(2):\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    image_ids = [\n",
    "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
    "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
    "                    ]\n",
    "                    all_image_ids.extend(image_ids)\n",
    "                    if not test_only:\n",
    "                        all_labels.extend(labels.numpy())\n",
    "                        \"\"\"\n",
    "\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Save predictions to csv file for Kaggle online evaluation\n",
    "    if test_only:\n",
    "        df = pd.DataFrame({\n",
    "            'ID': all_image_ids,\n",
    "            'TARGET': all_preds\n",
    "        })\n",
    "        df.to_csv(prediction_path, index=False)\n",
    "        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n",
    "    else:\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, per_class=False):\n",
    "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate and print precision and recall for each class\n",
    "    if per_class:\n",
    "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
    "\n",
    "    return kappa, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
    "                checkpoint_path='model.pth'):\n",
    "    best_model = model.state_dict()\n",
    "    best_epoch = None\n",
    "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "        running_loss = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
    "            for images, labels in train_loader:\n",
    "                if not isinstance(images, list):\n",
    "                    images = images.to(device)  # single image case\n",
    "                else:\n",
    "                    images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
    "                pbar.update(1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = sum(running_loss) / len(running_loss)\n",
    "\n",
    "        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n",
    "\n",
    "        kappa, accuracy, precision, recall = train_metrics[:4]\n",
    "        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
    "              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "        if len(train_metrics) > 4:\n",
    "            precision_per_class, recall_per_class = train_metrics[4:]\n",
    "            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n",
    "                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "        # Evaluation on the validation set at the end of each epoch\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
    "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
    "              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n",
    "\n",
    "        if val_kappa > best_val_kappa:\n",
    "            best_val_kappa = val_kappa\n",
    "            best_epoch = epoch\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, checkpoint_path)\n",
    "\n",
    "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # 5 DR levels\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "train_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/train_images/train_images/\"\n",
    "train_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/train_1.csv\"\n",
    "mode = 'single'\n",
    "\n",
    "train_dataset = RetinopathyDataset(ann_file=train_ann_file, image_dir=train_dir, mode=mode, transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/val_images/val_images/\"\n",
    "val_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/valid.csv\"\n",
    "mode = 'single'\n",
    "\n",
    "val_dataset = RetinopathyDataset(ann_file=val_ann_file, image_dir=val_dir, mode=mode, transform=transform_test)\n",
    "val_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Training: 100%|██████████| 92/92 [05:05<00:00,  3.32s/ batch, lr=1.0e-04, Loss=0.4952]\n",
      "[Train] Kappa: 0.5058 Accuracy: 0.6212 Precision: 0.5505 Recall: 0.6212 Loss: 1.0548\n",
      "[Train] Class 0: Precision: 0.7878, Recall: 0.8466\n",
      "[Train] Class 1: Precision: 0.1462, Recall: 0.0633\n",
      "[Train] Class 2: Precision: 0.5169, Recall: 0.7178\n",
      "[Train] Class 3: Precision: 0.0481, Recall: 0.0325\n",
      "[Train] Class 4: Precision: 0.0606, Recall: 0.0085\n",
      "Evaluating: 100%|██████████| 92/92 [04:50<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.6462 Accuracy: 0.7003 Precision: 0.5365 Recall: 0.7003\n",
      "\n",
      "Epoch 2/20\n",
      "Training: 100%|██████████| 92/92 [05:06<00:00,  3.33s/ batch, lr=1.0e-04, Loss=0.7283]\n",
      "[Train] Kappa: 0.7608 Accuracy: 0.7321 Precision: 0.6429 Recall: 0.7321 Loss: 0.7406\n",
      "[Train] Class 0: Precision: 0.9241, Recall: 0.9679\n",
      "[Train] Class 1: Precision: 0.4167, Recall: 0.1000\n",
      "[Train] Class 2: Precision: 0.5365, Recall: 0.8998\n",
      "[Train] Class 3: Precision: 0.0000, Recall: 0.0000\n",
      "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
      "Evaluating: 100%|██████████| 92/92 [04:52<00:00,  3.17s/ batch]\n",
      "[Val] Kappa: 0.7750 Accuracy: 0.7618 Precision: 0.6752 Recall: 0.7618\n",
      "\n",
      "Epoch 3/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.7568]\n",
      "[Train] Kappa: 0.7772 Accuracy: 0.7509 Precision: 0.7600 Recall: 0.7509 Loss: 0.6638\n",
      "[Train] Class 0: Precision: 0.9482, Recall: 0.9693\n",
      "[Train] Class 1: Precision: 0.4655, Recall: 0.2700\n",
      "[Train] Class 2: Precision: 0.5643, Recall: 0.8960\n",
      "[Train] Class 3: Precision: 0.7500, Recall: 0.0195\n",
      "[Train] Class 4: Precision: 0.6667, Recall: 0.0085\n",
      "Evaluating: 100%|██████████| 92/92 [04:48<00:00,  3.14s/ batch]\n",
      "[Val] Kappa: 0.7961 Accuracy: 0.7703 Precision: 0.6952 Recall: 0.7703\n",
      "\n",
      "Epoch 4/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.29s/ batch, lr=1.0e-04, Loss=0.3715]\n",
      "[Train] Kappa: 0.8017 Accuracy: 0.7597 Precision: 0.7290 Recall: 0.7597 Loss: 0.6293\n",
      "[Train] Class 0: Precision: 0.9569, Recall: 0.9763\n",
      "[Train] Class 1: Precision: 0.5243, Recall: 0.3600\n",
      "[Train] Class 2: Precision: 0.5798, Recall: 0.8589\n",
      "[Train] Class 3: Precision: 0.4545, Recall: 0.0974\n",
      "[Train] Class 4: Precision: 0.2903, Recall: 0.0385\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8186 Accuracy: 0.7850 Precision: 0.7715 Recall: 0.7850\n",
      "\n",
      "Epoch 5/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.4495]\n",
      "[Train] Kappa: 0.8273 Accuracy: 0.7730 Precision: 0.7542 Recall: 0.7730 Loss: 0.6044\n",
      "[Train] Class 0: Precision: 0.9595, Recall: 0.9742\n",
      "[Train] Class 1: Precision: 0.5291, Recall: 0.3933\n",
      "[Train] Class 2: Precision: 0.6162, Recall: 0.8564\n",
      "[Train] Class 3: Precision: 0.3818, Recall: 0.1364\n",
      "[Train] Class 4: Precision: 0.5068, Recall: 0.1581\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8697 Accuracy: 0.7788 Precision: 0.7858 Recall: 0.7788\n",
      "\n",
      "Epoch 6/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.31s/ batch, lr=1.0e-04, Loss=0.4822]\n",
      "[Train] Kappa: 0.8408 Accuracy: 0.7860 Precision: 0.7693 Recall: 0.7860 Loss: 0.5881\n",
      "[Train] Class 0: Precision: 0.9623, Recall: 0.9791\n",
      "[Train] Class 1: Precision: 0.5990, Recall: 0.3833\n",
      "[Train] Class 2: Precision: 0.6337, Recall: 0.8713\n",
      "[Train] Class 3: Precision: 0.4035, Recall: 0.1494\n",
      "[Train] Class 4: Precision: 0.5135, Recall: 0.2436\n",
      "Evaluating: 100%|██████████| 92/92 [04:50<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8615 Accuracy: 0.8051 Precision: 0.7931 Recall: 0.8051\n",
      "\n",
      "Epoch 7/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.6716]\n",
      "[Train] Kappa: 0.8634 Accuracy: 0.7969 Precision: 0.7795 Recall: 0.7969 Loss: 0.5452\n",
      "[Train] Class 0: Precision: 0.9737, Recall: 0.9805\n",
      "[Train] Class 1: Precision: 0.5848, Recall: 0.4367\n",
      "[Train] Class 2: Precision: 0.6497, Recall: 0.8700\n",
      "[Train] Class 3: Precision: 0.3250, Recall: 0.0844\n",
      "[Train] Class 4: Precision: 0.5857, Recall: 0.3504\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8617 Accuracy: 0.8123 Precision: 0.8152 Recall: 0.8123\n",
      "\n",
      "Epoch 8/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.4357]\n",
      "[Train] Kappa: 0.8532 Accuracy: 0.7966 Precision: 0.7824 Recall: 0.7966 Loss: 0.5303\n",
      "[Train] Class 0: Precision: 0.9688, Recall: 0.9742\n",
      "[Train] Class 1: Precision: 0.5817, Recall: 0.4867\n",
      "[Train] Class 2: Precision: 0.6712, Recall: 0.8465\n",
      "[Train] Class 3: Precision: 0.4154, Recall: 0.1753\n",
      "[Train] Class 4: Precision: 0.5229, Recall: 0.3419\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.14s/ batch]\n",
      "[Val] Kappa: 0.8817 Accuracy: 0.8232 Precision: 0.8247 Recall: 0.8232\n",
      "\n",
      "Epoch 9/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.4986]\n",
      "[Train] Kappa: 0.8731 Accuracy: 0.8038 Precision: 0.7900 Recall: 0.8038 Loss: 0.5197\n",
      "[Train] Class 0: Precision: 0.9743, Recall: 0.9777\n",
      "[Train] Class 1: Precision: 0.6084, Recall: 0.5333\n",
      "[Train] Class 2: Precision: 0.6856, Recall: 0.8366\n",
      "[Train] Class 3: Precision: 0.3125, Recall: 0.1623\n",
      "[Train] Class 4: Precision: 0.5679, Recall: 0.3932\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8808 Accuracy: 0.8290 Precision: 0.8351 Recall: 0.8290\n",
      "\n",
      "Epoch 10/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-04, Loss=0.4021]\n",
      "[Train] Kappa: 0.8666 Accuracy: 0.8143 Precision: 0.8046 Recall: 0.8143 Loss: 0.5066\n",
      "[Train] Class 0: Precision: 0.9737, Recall: 0.9812\n",
      "[Train] Class 1: Precision: 0.6352, Recall: 0.4933\n",
      "[Train] Class 2: Precision: 0.6845, Recall: 0.8700\n",
      "[Train] Class 3: Precision: 0.5283, Recall: 0.1818\n",
      "[Train] Class 4: Precision: 0.5814, Recall: 0.4274\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.8913 Accuracy: 0.8334 Precision: 0.8251 Recall: 0.8334\n",
      "\n",
      "Epoch 11/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.4925]\n",
      "[Train] Kappa: 0.8855 Accuracy: 0.8195 Precision: 0.8096 Recall: 0.8195 Loss: 0.4814\n",
      "[Train] Class 0: Precision: 0.9779, Recall: 0.9854\n",
      "[Train] Class 1: Precision: 0.6367, Recall: 0.5433\n",
      "[Train] Class 2: Precision: 0.7073, Recall: 0.8403\n",
      "[Train] Class 3: Precision: 0.4409, Recall: 0.2662\n",
      "[Train] Class 4: Precision: 0.5966, Recall: 0.4487\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9046 Accuracy: 0.8495 Precision: 0.8469 Recall: 0.8495\n",
      "\n",
      "Epoch 12/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.3255]\n",
      "[Train] Kappa: 0.9034 Accuracy: 0.8365 Precision: 0.8277 Recall: 0.8365 Loss: 0.4447\n",
      "[Train] Class 0: Precision: 0.9820, Recall: 0.9895\n",
      "[Train] Class 1: Precision: 0.6600, Recall: 0.5500\n",
      "[Train] Class 2: Precision: 0.7220, Recall: 0.8775\n",
      "[Train] Class 3: Precision: 0.4691, Recall: 0.2468\n",
      "[Train] Class 4: Precision: 0.6977, Recall: 0.5128\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9089 Accuracy: 0.8560 Precision: 0.8521 Recall: 0.8560\n",
      "\n",
      "Epoch 13/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.1707]\n",
      "[Train] Kappa: 0.8977 Accuracy: 0.8300 Precision: 0.8225 Recall: 0.8300 Loss: 0.4387\n",
      "[Train] Class 0: Precision: 0.9833, Recall: 0.9881\n",
      "[Train] Class 1: Precision: 0.6749, Recall: 0.5467\n",
      "[Train] Class 2: Precision: 0.7026, Recall: 0.8626\n",
      "[Train] Class 3: Precision: 0.4875, Recall: 0.2532\n",
      "[Train] Class 4: Precision: 0.6609, Recall: 0.4915\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9143 Accuracy: 0.8608 Precision: 0.8587 Recall: 0.8608\n",
      "\n",
      "Epoch 14/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.4524]\n",
      "[Train] Kappa: 0.8991 Accuracy: 0.8406 Precision: 0.8359 Recall: 0.8406 Loss: 0.4305\n",
      "[Train] Class 0: Precision: 0.9813, Recall: 0.9881\n",
      "[Train] Class 1: Precision: 0.6864, Recall: 0.5400\n",
      "[Train] Class 2: Precision: 0.7160, Recall: 0.8923\n",
      "[Train] Class 3: Precision: 0.6133, Recall: 0.2987\n",
      "[Train] Class 4: Precision: 0.6964, Recall: 0.5000\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9176 Accuracy: 0.8628 Precision: 0.8600 Recall: 0.8628\n",
      "\n",
      "Epoch 15/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.5307]\n",
      "[Train] Kappa: 0.9041 Accuracy: 0.8502 Precision: 0.8451 Recall: 0.8502 Loss: 0.4133\n",
      "[Train] Class 0: Precision: 0.9847, Recall: 0.9902\n",
      "[Train] Class 1: Precision: 0.7333, Recall: 0.5867\n",
      "[Train] Class 2: Precision: 0.7315, Recall: 0.9072\n",
      "[Train] Class 3: Precision: 0.6026, Recall: 0.3052\n",
      "[Train] Class 4: Precision: 0.6845, Recall: 0.4915\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9167 Accuracy: 0.8614 Precision: 0.8584 Recall: 0.8614\n",
      "\n",
      "Epoch 16/20\n",
      "Training: 100%|██████████| 92/92 [05:03<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.2783]\n",
      "[Train] Kappa: 0.9022 Accuracy: 0.8403 Precision: 0.8320 Recall: 0.8403 Loss: 0.4262\n",
      "[Train] Class 0: Precision: 0.9854, Recall: 0.9888\n",
      "[Train] Class 1: Precision: 0.7131, Recall: 0.5800\n",
      "[Train] Class 2: Precision: 0.7254, Recall: 0.8861\n",
      "[Train] Class 3: Precision: 0.4875, Recall: 0.2532\n",
      "[Train] Class 4: Precision: 0.6389, Recall: 0.4915\n",
      "Evaluating: 100%|██████████| 92/92 [04:49<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9176 Accuracy: 0.8703 Precision: 0.8680 Recall: 0.8703\n",
      "\n",
      "Epoch 17/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.31s/ batch, lr=1.0e-05, Loss=0.3456]\n",
      "[Train] Kappa: 0.8971 Accuracy: 0.8403 Precision: 0.8323 Recall: 0.8403 Loss: 0.4158\n",
      "[Train] Class 0: Precision: 0.9813, Recall: 0.9868\n",
      "[Train] Class 1: Precision: 0.6996, Recall: 0.5900\n",
      "[Train] Class 2: Precision: 0.7280, Recall: 0.8812\n",
      "[Train] Class 3: Precision: 0.5195, Recall: 0.2597\n",
      "[Train] Class 4: Precision: 0.6556, Recall: 0.5043\n",
      "Evaluating: 100%|██████████| 92/92 [04:50<00:00,  3.16s/ batch]\n",
      "[Val] Kappa: 0.9207 Accuracy: 0.8645 Precision: 0.8599 Recall: 0.8645\n",
      "\n",
      "Epoch 18/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.31s/ batch, lr=1.0e-05, Loss=0.2050]\n",
      "[Train] Kappa: 0.9129 Accuracy: 0.8532 Precision: 0.8482 Recall: 0.8532 Loss: 0.4074\n",
      "[Train] Class 0: Precision: 0.9820, Recall: 0.9868\n",
      "[Train] Class 1: Precision: 0.7099, Recall: 0.6200\n",
      "[Train] Class 2: Precision: 0.7542, Recall: 0.8849\n",
      "[Train] Class 3: Precision: 0.5922, Recall: 0.3961\n",
      "[Train] Class 4: Precision: 0.6989, Recall: 0.5256\n",
      "Evaluating: 100%|██████████| 92/92 [04:51<00:00,  3.17s/ batch]\n",
      "[Val] Kappa: 0.9147 Accuracy: 0.8635 Precision: 0.8591 Recall: 0.8635\n",
      "\n",
      "Epoch 19/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.31s/ batch, lr=1.0e-05, Loss=0.4898]\n",
      "[Train] Kappa: 0.9055 Accuracy: 0.8410 Precision: 0.8341 Recall: 0.8410 Loss: 0.4186\n",
      "[Train] Class 0: Precision: 0.9827, Recall: 0.9888\n",
      "[Train] Class 1: Precision: 0.6939, Recall: 0.5667\n",
      "[Train] Class 2: Precision: 0.7217, Recall: 0.8762\n",
      "[Train] Class 3: Precision: 0.5256, Recall: 0.2662\n",
      "[Train] Class 4: Precision: 0.6940, Recall: 0.5427\n",
      "Evaluating: 100%|██████████| 92/92 [04:50<00:00,  3.15s/ batch]\n",
      "[Val] Kappa: 0.9187 Accuracy: 0.8730 Precision: 0.8709 Recall: 0.8730\n",
      "\n",
      "Epoch 20/20\n",
      "Training: 100%|██████████| 92/92 [05:04<00:00,  3.30s/ batch, lr=1.0e-05, Loss=0.3107]\n",
      "[Train] Kappa: 0.9059 Accuracy: 0.8447 Precision: 0.8371 Recall: 0.8447 Loss: 0.4106\n",
      "[Train] Class 0: Precision: 0.9841, Recall: 0.9923\n",
      "[Train] Class 1: Precision: 0.7218, Recall: 0.5967\n",
      "[Train] Class 2: Precision: 0.7327, Recall: 0.8787\n",
      "[Train] Class 3: Precision: 0.5000, Recall: 0.2922\n",
      "[Train] Class 4: Precision: 0.6667, Recall: 0.5043\n",
      "Evaluating: 100%|██████████| 92/92 [04:50<00:00,  3.16s/ batch]\n",
      "[Val] Kappa: 0.9234 Accuracy: 0.8717 Precision: 0.8694 Recall: 0.8717\n",
      "[Val] Best kappa: 0.9234, Epoch 20\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model.to(device)\n",
    "\n",
    "#for param in model.backbone.parameters():\n",
    "#    param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model = train_model(\n",
    "        model, train_loader, val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "        checkpoint_path='/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/models/train_with_APTOS.pth'\n",
    "    )\n",
    "\n",
    "\n",
    "# Best [Val] Kappa: 0.9234 Accuracy: 0.8717 Precision: 0.8694 Recall: 0.8717 Epoch 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Results of Single imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_test(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_image_ids = []\n",
    "\n",
    "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            #if test_only:\n",
    "            #    images = data\n",
    "            #else:\n",
    "            #    images, labels = data\n",
    "\n",
    "            images, labels = data\n",
    "            \n",
    "            if not isinstance(images, list):\n",
    "                images = images.to(device)  # single image case\n",
    "            else:\n",
    "                images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                # single image case\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "                \"\"\"\n",
    "                image_ids = [\n",
    "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
    "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
    "                ]\n",
    "                all_image_ids.extend(image_ids)\n",
    "                if not test_only:\n",
    "                    all_labels.extend(labels.numpy())\n",
    "            else:\n",
    "                # dual images case\n",
    "                for k in range(2):\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    image_ids = [\n",
    "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
    "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
    "                    ]\n",
    "                    all_image_ids.extend(image_ids)\n",
    "                    if not test_only:\n",
    "                        all_labels.extend(labels.numpy())\n",
    "                        \"\"\"\n",
    "\n",
    "            pbar.update(1)\n",
    "    \"\"\"\n",
    "    # Save predictions to csv file for Kaggle online evaluation\n",
    "    if test_only:\n",
    "        df = pd.DataFrame({\n",
    "            'ID': all_image_ids,\n",
    "            'TARGET': all_preds\n",
    "        })\n",
    "        df.to_csv(prediction_path, index=False)\n",
    "        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n",
    "    else:\n",
    "        \"\"\"\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, per_class=False):\n",
    "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate and print precision and recall for each class\n",
    "    if per_class:\n",
    "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
    "\n",
    "    return kappa, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/t87m92j14g9b1yw35n324r5w0000gn/T/ipykernel_49062/2476310960.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(dict_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/test_images/test_images/\"\n",
    "test_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/APTOS/test.csv\"\n",
    "mode = 'single'\n",
    "\n",
    "test_dataset = RetinopathyDataset(ann_file=test_ann_file, image_dir=test_dir, mode=mode, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dict_path = '/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/models/train_with_APTOS.pth'\n",
    "\n",
    "state_dict = torch.load(dict_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:43<00:00,  3.59s/ batch]\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/APTOS_test_result.csv\"\n",
    "kappa, accuracy, precision, recall = evaluate_model_test(model, test_loader, device, test_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Kappa: 0.8908 Accuracy: 0.8251 Precision: 0.8370 Recall: 0.8251\n"
     ]
    }
   ],
   "source": [
    "print(f'[Test] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
    "              f'Precision: {precision:.4f} Recall: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
