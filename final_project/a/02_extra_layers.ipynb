{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = 24\n",
    "num_classes = 5  # 5 DR levelsA\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetinopathyDataset(Dataset):\n",
    "    def __init__(self, ann_file, image_dir, transform=None, mode='single', test=False):\n",
    "        self.ann_file = ann_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.test = test\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'single':\n",
    "            self.data = self.load_data()\n",
    "        else:\n",
    "            self.data = self.load_data_dual()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'single':\n",
    "            return self.get_item(index)\n",
    "        else:\n",
    "            return self.get_item_dual(index)\n",
    "\n",
    "    # 1. single image\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_info = dict()\n",
    "            file_info['img_path'] = os.path.join(self.image_dir, row['img_path'])\n",
    "            if not self.test:\n",
    "                file_info['dr_level'] = int(row['patient_DR_Level'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item(self, index):\n",
    "        data = self.data[index]\n",
    "        img = Image.open(data['img_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if not self.test:\n",
    "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "            return img, label\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    # 2. dual image\n",
    "    def load_data_dual(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        df['prefix'] = df['image_id'].str.split('_').str[0]  # The patient id of each image\n",
    "        df['suffix'] = df['image_id'].str.split('_').str[1].str[0]  # The left or right eye\n",
    "        grouped = df.groupby(['prefix', 'suffix'])\n",
    "\n",
    "        data = []\n",
    "        for (prefix, suffix), group in grouped:\n",
    "            file_info = dict()\n",
    "            file_info['img_path1'] = os.path.join(self.image_dir, group.iloc[0]['img_path'])\n",
    "            file_info['img_path2'] = os.path.join(self.image_dir, group.iloc[1]['img_path'])\n",
    "            if not self.test:\n",
    "                file_info['dr_level'] = int(group.iloc[0]['patient_DR_Level'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item_dual(self, index):\n",
    "        data = self.data[index]\n",
    "        img1 = Image.open(data['img_path1']).convert('RGB')\n",
    "        img2 = Image.open(data['img_path2']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        if not self.test:\n",
    "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "            return [img1, img2], label\n",
    "        else:\n",
    "            return [img1, img2]\n",
    "\n",
    "\n",
    "class CutOut(object):\n",
    "    def __init__(self, mask_size, p=0.5):\n",
    "        self.mask_size = mask_size\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() > self.p:\n",
    "            return img\n",
    "\n",
    "        # Ensure the image is a tensor\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            raise TypeError('Input image must be a torch.Tensor')\n",
    "\n",
    "        # Get height and width of the image\n",
    "        h, w = img.shape[1], img.shape[2]\n",
    "        mask_size_half = self.mask_size // 2\n",
    "        offset = 1 if self.mask_size % 2 == 0 else 0\n",
    "\n",
    "        cx = np.random.randint(mask_size_half, w + offset - mask_size_half)\n",
    "        cy = np.random.randint(mask_size_half, h + offset - mask_size_half)\n",
    "\n",
    "        xmin, xmax = cx - mask_size_half, cx + mask_size_half + offset\n",
    "        ymin, ymax = cy - mask_size_half, cy + mask_size_half + offset\n",
    "        xmin, xmax = max(0, xmin), min(w, xmax)\n",
    "        ymin, ymax = max(0, ymin), min(h, ymax)\n",
    "\n",
    "        img[:, ymin:ymax, xmin:xmax] = 0\n",
    "        return img\n",
    "\n",
    "\n",
    "class SLORandomPad:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        pad_width = max(0, self.size[0] - img.width)\n",
    "        pad_height = max(0, self.size[1] - img.height)\n",
    "        pad_left = random.randint(0, pad_width)\n",
    "        pad_top = random.randint(0, pad_height)\n",
    "        pad_right = pad_width - pad_left\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n",
    "\n",
    "\n",
    "class FundRandomRotate:\n",
    "    def __init__(self, prob, degree):\n",
    "        self.prob = prob\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.prob:\n",
    "            angle = random.uniform(-self.degree, self.degree)\n",
    "            return transforms.functional.rotate(img, angle)\n",
    "        return img\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop((210, 210)),\n",
    "    SLORandomPad((224, 224)),\n",
    "    FundRandomRotate(prob=0.5, degree=30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
    "                checkpoint_path='model.pth'):\n",
    "    best_model = model.state_dict()\n",
    "    best_epoch = None\n",
    "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "        running_loss = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
    "            for images, labels in train_loader:\n",
    "                if not isinstance(images, list):\n",
    "                    images = images.to(device)  # single image case\n",
    "                else:\n",
    "                    images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
    "                pbar.update(1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = sum(running_loss) / len(running_loss)\n",
    "\n",
    "        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n",
    "        kappa, accuracy, precision, recall = train_metrics[:4]\n",
    "\n",
    "        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
    "              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        if len(train_metrics) > 4:\n",
    "            precision_per_class, recall_per_class = train_metrics[4:]\n",
    "            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n",
    "                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "        # Evaluation on the validation set at the end of each epoch\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
    "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
    "              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n",
    "\n",
    "        if val_kappa > best_val_kappa:\n",
    "            best_val_kappa = val_kappa\n",
    "            best_epoch = epoch\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, checkpoint_path)\n",
    "\n",
    "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_image_ids = []\n",
    "\n",
    "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            if test_only:\n",
    "                images = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                images = images.to(device)  # single image case\n",
    "            else:\n",
    "                images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                # single image case\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                image_ids = [\n",
    "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
    "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
    "                ]\n",
    "                all_image_ids.extend(image_ids)\n",
    "                if not test_only:\n",
    "                    all_labels.extend(labels.numpy())\n",
    "            else:\n",
    "                # dual images case\n",
    "                for k in range(2):\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    image_ids = [\n",
    "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
    "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
    "                    ]\n",
    "                    all_image_ids.extend(image_ids)\n",
    "                    if not test_only:\n",
    "                        all_labels.extend(labels.numpy())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save predictions to csv file for Kaggle online evaluation\n",
    "    if test_only:\n",
    "        df = pd.DataFrame({\n",
    "            'ID': all_image_ids,\n",
    "            'TARGET': all_preds\n",
    "        })\n",
    "        df.to_csv(prediction_path, index=False)\n",
    "        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n",
    "    else:\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, per_class=False):\n",
    "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate and print precision and recall for each class\n",
    "    if per_class:\n",
    "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
    "\n",
    "    return kappa, accuracy, precision, recall\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()  # Remove the original classification layer\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyDualModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "        backbone.fc = nn.Identity()\n",
    "\n",
    "        # Here the two backbones will have the same structure but unshared weights\n",
    "        self.backbone1 = copy.deepcopy(backbone)\n",
    "        self.backbone2 = copy.deepcopy(backbone)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        image1, image2 = images\n",
    "\n",
    "        x1 = self.backbone1(image1)\n",
    "        x2 = self.backbone2(image2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=5, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Pipeline Mode: single\n",
      "Device: mps\n",
      "\n",
      "Epoch 1/25\n",
      "Training: 100%|██████████| 50/50 [00:15<00:00,  3.21 batch/s, lr=1.0e-04, Loss=1.5390]\n",
      "[Train] Kappa: 0.1207 Accuracy: 0.2633 Precision: 0.2795 Recall: 0.2633 Loss: 1.5618\n",
      "[Train] Class 0: Precision: 0.4638, Recall: 0.3556\n",
      "[Train] Class 1: Precision: 0.1802, Recall: 0.3042\n",
      "[Train] Class 2: Precision: 0.2213, Recall: 0.4250\n",
      "[Train] Class 3: Precision: 0.2449, Recall: 0.0500\n",
      "[Train] Class 4: Precision: 0.1111, Recall: 0.0083\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.16 batch/s]\n",
      "[Val] Kappa: 0.3145 Accuracy: 0.4225 Precision: 0.2744 Recall: 0.4225\n",
      "\n",
      "Epoch 2/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.55 batch/s, lr=1.0e-04, Loss=1.0788]\n",
      "[Train] Kappa: 0.4658 Accuracy: 0.4483 Precision: 0.3840 Recall: 0.4483 Loss: 1.3233\n",
      "[Train] Class 0: Precision: 0.7168, Recall: 0.9139\n",
      "[Train] Class 1: Precision: 0.2792, Recall: 0.2792\n",
      "[Train] Class 2: Precision: 0.2853, Recall: 0.4375\n",
      "[Train] Class 3: Precision: 0.2803, Recall: 0.1542\n",
      "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.18 batch/s]\n",
      "[Val] Kappa: 0.6990 Accuracy: 0.5525 Precision: 0.5266 Recall: 0.5525\n",
      "\n",
      "Epoch 3/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.56 batch/s, lr=1.0e-04, Loss=1.2481]\n",
      "[Train] Kappa: 0.6126 Accuracy: 0.4942 Precision: 0.4885 Recall: 0.4942 Loss: 1.2271\n",
      "[Train] Class 0: Precision: 0.7727, Recall: 0.9444\n",
      "[Train] Class 1: Precision: 0.3804, Recall: 0.2917\n",
      "[Train] Class 2: Precision: 0.2791, Recall: 0.4000\n",
      "[Train] Class 3: Precision: 0.3739, Recall: 0.3583\n",
      "[Train] Class 4: Precision: 0.5000, Recall: 0.0083\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.56 batch/s]\n",
      "[Val] Kappa: 0.7021 Accuracy: 0.5225 Precision: 0.5165 Recall: 0.5225\n",
      "\n",
      "Epoch 4/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.57 batch/s, lr=1.0e-04, Loss=0.9091]\n",
      "[Train] Kappa: 0.6892 Accuracy: 0.5283 Precision: 0.4632 Recall: 0.5283 Loss: 1.1480\n",
      "[Train] Class 0: Precision: 0.7798, Recall: 0.9444\n",
      "[Train] Class 1: Precision: 0.4587, Recall: 0.4625\n",
      "[Train] Class 2: Precision: 0.2876, Recall: 0.2708\n",
      "[Train] Class 3: Precision: 0.4000, Recall: 0.4917\n",
      "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.62 batch/s]\n",
      "[Val] Kappa: 0.6340 Accuracy: 0.5650 Precision: 0.4862 Recall: 0.5650\n",
      "\n",
      "Epoch 5/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.53 batch/s, lr=1.0e-04, Loss=1.3316]\n",
      "[Train] Kappa: 0.7489 Accuracy: 0.5775 Precision: 0.5422 Recall: 0.5775 Loss: 1.0422\n",
      "[Train] Class 0: Precision: 0.8122, Recall: 0.9611\n",
      "[Train] Class 1: Precision: 0.5046, Recall: 0.4542\n",
      "[Train] Class 2: Precision: 0.3302, Recall: 0.2958\n",
      "[Train] Class 3: Precision: 0.4910, Recall: 0.6833\n",
      "[Train] Class 4: Precision: 0.3333, Recall: 0.0250\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.13 batch/s]\n",
      "[Val] Kappa: 0.7667 Accuracy: 0.5800 Precision: 0.5065 Recall: 0.5800\n",
      "\n",
      "Epoch 6/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.61 batch/s, lr=1.0e-04, Loss=1.3094]\n",
      "[Train] Kappa: 0.7765 Accuracy: 0.6017 Precision: 0.5666 Recall: 0.6017 Loss: 0.9798\n",
      "[Train] Class 0: Precision: 0.8208, Recall: 0.9667\n",
      "[Train] Class 1: Precision: 0.5542, Recall: 0.5750\n",
      "[Train] Class 2: Precision: 0.3590, Recall: 0.2333\n",
      "[Train] Class 3: Precision: 0.4888, Recall: 0.7250\n",
      "[Train] Class 4: Precision: 0.4000, Recall: 0.0500\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.57 batch/s]\n",
      "[Val] Kappa: 0.7023 Accuracy: 0.6075 Precision: 0.5286 Recall: 0.6075\n",
      "\n",
      "Epoch 7/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.53 batch/s, lr=1.0e-04, Loss=0.8547]\n",
      "[Train] Kappa: 0.8046 Accuracy: 0.6142 Precision: 0.5805 Recall: 0.6142 Loss: 0.9339\n",
      "[Train] Class 0: Precision: 0.8506, Recall: 0.9806\n",
      "[Train] Class 1: Precision: 0.5622, Recall: 0.5833\n",
      "[Train] Class 2: Precision: 0.3922, Recall: 0.2500\n",
      "[Train] Class 3: Precision: 0.4985, Recall: 0.7000\n",
      "[Train] Class 4: Precision: 0.3478, Recall: 0.1333\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.54 batch/s]\n",
      "[Val] Kappa: 0.7712 Accuracy: 0.6175 Precision: 0.5419 Recall: 0.6175\n",
      "\n",
      "Epoch 8/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.67 batch/s, lr=1.0e-04, Loss=0.8335]\n",
      "[Train] Kappa: 0.8053 Accuracy: 0.6325 Precision: 0.6097 Recall: 0.6325 Loss: 0.9110\n",
      "[Train] Class 0: Precision: 0.8564, Recall: 0.9778\n",
      "[Train] Class 1: Precision: 0.5992, Recall: 0.6042\n",
      "[Train] Class 2: Precision: 0.4557, Recall: 0.3000\n",
      "[Train] Class 3: Precision: 0.4918, Recall: 0.7500\n",
      "[Train] Class 4: Precision: 0.4348, Recall: 0.0833\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.00 batch/s]\n",
      "[Val] Kappa: 0.7385 Accuracy: 0.6225 Precision: 0.5482 Recall: 0.6225\n",
      "\n",
      "Epoch 9/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.65 batch/s, lr=1.0e-04, Loss=0.7432]\n",
      "[Train] Kappa: 0.8084 Accuracy: 0.6300 Precision: 0.5912 Recall: 0.6300 Loss: 0.8893\n",
      "[Train] Class 0: Precision: 0.8614, Recall: 0.9667\n",
      "[Train] Class 1: Precision: 0.5913, Recall: 0.6208\n",
      "[Train] Class 2: Precision: 0.4560, Recall: 0.3458\n",
      "[Train] Class 3: Precision: 0.5029, Recall: 0.7125\n",
      "[Train] Class 4: Precision: 0.2273, Recall: 0.0417\n",
      "Evaluating: 100%|██████████| 17/17 [00:01<00:00,  8.77 batch/s]\n",
      "[Val] Kappa: 0.8026 Accuracy: 0.6325 Precision: 0.5669 Recall: 0.6325\n",
      "\n",
      "Epoch 10/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.44 batch/s, lr=1.0e-04, Loss=0.7574]\n",
      "[Train] Kappa: 0.8203 Accuracy: 0.6483 Precision: 0.6229 Recall: 0.6483 Loss: 0.8467\n",
      "[Train] Class 0: Precision: 0.8540, Recall: 0.9583\n",
      "[Train] Class 1: Precision: 0.6682, Recall: 0.6125\n",
      "[Train] Class 2: Precision: 0.4769, Recall: 0.3875\n",
      "[Train] Class 3: Precision: 0.5262, Recall: 0.7542\n",
      "[Train] Class 4: Precision: 0.3243, Recall: 0.1000\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.54 batch/s]\n",
      "[Val] Kappa: 0.8076 Accuracy: 0.6200 Precision: 0.5389 Recall: 0.6200\n",
      "\n",
      "Epoch 11/25\n",
      "Training: 100%|██████████| 50/50 [00:14<00:00,  3.56 batch/s, lr=1.0e-05, Loss=0.7802]\n",
      "[Train] Kappa: 0.8312 Accuracy: 0.6667 Precision: 0.6406 Recall: 0.6667 Loss: 0.8105\n",
      "[Train] Class 0: Precision: 0.8728, Recall: 0.9722\n",
      "[Train] Class 1: Precision: 0.6233, Recall: 0.7792\n",
      "[Train] Class 2: Precision: 0.4967, Recall: 0.3167\n",
      "[Train] Class 3: Precision: 0.5541, Recall: 0.7042\n",
      "[Train] Class 4: Precision: 0.4390, Recall: 0.1500\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.22 batch/s]\n",
      "[Val] Kappa: 0.7107 Accuracy: 0.5925 Precision: 0.4959 Recall: 0.5925\n",
      "\n",
      "Epoch 12/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.63 batch/s, lr=1.0e-05, Loss=0.8058]\n",
      "[Train] Kappa: 0.8391 Accuracy: 0.6858 Precision: 0.6724 Recall: 0.6858 Loss: 0.7847\n",
      "[Train] Class 0: Precision: 0.8537, Recall: 0.9722\n",
      "[Train] Class 1: Precision: 0.6784, Recall: 0.7208\n",
      "[Train] Class 2: Precision: 0.5538, Recall: 0.4292\n",
      "[Train] Class 3: Precision: 0.5637, Recall: 0.7375\n",
      "[Train] Class 4: Precision: 0.5714, Recall: 0.1667\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.02 batch/s]\n",
      "[Val] Kappa: 0.7798 Accuracy: 0.6250 Precision: 0.5464 Recall: 0.6250\n",
      "\n",
      "Epoch 13/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.67 batch/s, lr=1.0e-05, Loss=1.2477]\n",
      "[Train] Kappa: 0.8666 Accuracy: 0.6950 Precision: 0.6759 Recall: 0.6950 Loss: 0.7724\n",
      "[Train] Class 0: Precision: 0.8853, Recall: 0.9861\n",
      "[Train] Class 1: Precision: 0.6848, Recall: 0.7333\n",
      "[Train] Class 2: Precision: 0.5568, Recall: 0.4083\n",
      "[Train] Class 3: Precision: 0.5676, Recall: 0.7875\n",
      "[Train] Class 4: Precision: 0.4848, Recall: 0.1333\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.18 batch/s]\n",
      "[Val] Kappa: 0.7532 Accuracy: 0.6025 Precision: 0.5193 Recall: 0.6025\n",
      "\n",
      "Epoch 14/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.61 batch/s, lr=1.0e-05, Loss=0.8637]\n",
      "[Train] Kappa: 0.8582 Accuracy: 0.6850 Precision: 0.6652 Recall: 0.6850 Loss: 0.7574\n",
      "[Train] Class 0: Precision: 0.9082, Recall: 0.9889\n",
      "[Train] Class 1: Precision: 0.6926, Recall: 0.7417\n",
      "[Train] Class 2: Precision: 0.5537, Recall: 0.4083\n",
      "[Train] Class 3: Precision: 0.5260, Recall: 0.7167\n",
      "[Train] Class 4: Precision: 0.3830, Recall: 0.1500\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.83 batch/s]\n",
      "[Val] Kappa: 0.7746 Accuracy: 0.6300 Precision: 0.5533 Recall: 0.6300\n",
      "\n",
      "Epoch 15/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.61 batch/s, lr=1.0e-05, Loss=0.5550]\n",
      "[Train] Kappa: 0.8687 Accuracy: 0.7025 Precision: 0.6838 Recall: 0.7025 Loss: 0.7384\n",
      "[Train] Class 0: Precision: 0.8987, Recall: 0.9861\n",
      "[Train] Class 1: Precision: 0.7288, Recall: 0.7167\n",
      "[Train] Class 2: Precision: 0.5707, Recall: 0.4708\n",
      "[Train] Class 3: Precision: 0.5592, Recall: 0.7875\n",
      "[Train] Class 4: Precision: 0.4242, Recall: 0.1167\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.56 batch/s]\n",
      "[Val] Kappa: 0.7764 Accuracy: 0.6250 Precision: 0.5465 Recall: 0.6250\n",
      "\n",
      "Epoch 16/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.60 batch/s, lr=1.0e-05, Loss=0.8060]\n",
      "[Train] Kappa: 0.8748 Accuracy: 0.7075 Precision: 0.6909 Recall: 0.7075 Loss: 0.7414\n",
      "[Train] Class 0: Precision: 0.8931, Recall: 0.9750\n",
      "[Train] Class 1: Precision: 0.6988, Recall: 0.7250\n",
      "[Train] Class 2: Precision: 0.6186, Recall: 0.5000\n",
      "[Train] Class 3: Precision: 0.5736, Recall: 0.7792\n",
      "[Train] Class 4: Precision: 0.4474, Recall: 0.1417\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.51 batch/s]\n",
      "[Val] Kappa: 0.7624 Accuracy: 0.6175 Precision: 0.5409 Recall: 0.6175\n",
      "\n",
      "Epoch 17/25\n",
      "Training: 100%|██████████| 50/50 [00:13<00:00,  3.63 batch/s, lr=1.0e-05, Loss=0.6312]\n",
      "[Train] Kappa: 0.8775 Accuracy: 0.7142 Precision: 0.7061 Recall: 0.7142 Loss: 0.6928\n",
      "[Train] Class 0: Precision: 0.9026, Recall: 0.9778\n",
      "[Train] Class 1: Precision: 0.7154, Recall: 0.7542\n",
      "[Train] Class 2: Precision: 0.6215, Recall: 0.4583\n",
      "[Train] Class 3: Precision: 0.5641, Recall: 0.8250\n",
      "[Train] Class 4: Precision: 0.5517, Recall: 0.1333\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  7.99 batch/s]\n",
      "[Val] Kappa: 0.7927 Accuracy: 0.6275 Precision: 0.5497 Recall: 0.6275\n",
      "\n",
      "Epoch 18/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.27 batch/s, lr=1.0e-05, Loss=0.5839]\n",
      "[Train] Kappa: 0.8731 Accuracy: 0.7125 Precision: 0.7019 Recall: 0.7125 Loss: 0.7122\n",
      "[Train] Class 0: Precision: 0.9054, Recall: 0.9833\n",
      "[Train] Class 1: Precision: 0.7404, Recall: 0.7250\n",
      "[Train] Class 2: Precision: 0.6150, Recall: 0.5125\n",
      "[Train] Class 3: Precision: 0.5522, Recall: 0.7708\n",
      "[Train] Class 4: Precision: 0.4872, Recall: 0.1583\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.19 batch/s]\n",
      "[Val] Kappa: 0.8061 Accuracy: 0.6400 Precision: 0.5633 Recall: 0.6400\n",
      "\n",
      "Epoch 19/25\n",
      "Training: 100%|██████████| 50/50 [00:12<00:00,  3.87 batch/s, lr=1.0e-05, Loss=0.5907]\n",
      "[Train] Kappa: 0.8759 Accuracy: 0.7117 Precision: 0.7054 Recall: 0.7117 Loss: 0.7155\n",
      "[Train] Class 0: Precision: 0.9028, Recall: 0.9806\n",
      "[Train] Class 1: Precision: 0.7056, Recall: 0.7292\n",
      "[Train] Class 2: Precision: 0.6073, Recall: 0.4833\n",
      "[Train] Class 3: Precision: 0.5644, Recall: 0.7667\n",
      "[Train] Class 4: Precision: 0.5909, Recall: 0.2167\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  6.29 batch/s]\n",
      "[Val] Kappa: 0.8186 Accuracy: 0.6450 Precision: 0.5712 Recall: 0.6450\n",
      "\n",
      "Epoch 20/25\n",
      "Training: 100%|██████████| 50/50 [00:12<00:00,  4.16 batch/s, lr=1.0e-05, Loss=0.7904]\n",
      "[Train] Kappa: 0.8780 Accuracy: 0.7375 Precision: 0.7298 Recall: 0.7375 Loss: 0.6719\n",
      "[Train] Class 0: Precision: 0.9119, Recall: 0.9778\n",
      "[Train] Class 1: Precision: 0.7390, Recall: 0.7667\n",
      "[Train] Class 2: Precision: 0.6533, Recall: 0.5417\n",
      "[Train] Class 3: Precision: 0.6012, Recall: 0.8167\n",
      "[Train] Class 4: Precision: 0.5750, Recall: 0.1917\n",
      "Evaluating: 100%|██████████| 17/17 [00:01<00:00,  8.68 batch/s]\n",
      "[Val] Kappa: 0.8286 Accuracy: 0.6475 Precision: 0.6732 Recall: 0.6475\n",
      "\n",
      "Epoch 21/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.28 batch/s, lr=1.0e-06, Loss=0.4869]\n",
      "[Train] Kappa: 0.8849 Accuracy: 0.7292 Precision: 0.7163 Recall: 0.7292 Loss: 0.6575\n",
      "[Train] Class 0: Precision: 0.9177, Recall: 0.9917\n",
      "[Train] Class 1: Precision: 0.7712, Recall: 0.7583\n",
      "[Train] Class 2: Precision: 0.6184, Recall: 0.5333\n",
      "[Train] Class 3: Precision: 0.5833, Recall: 0.7583\n",
      "[Train] Class 4: Precision: 0.4643, Recall: 0.2167\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.31 batch/s]\n",
      "[Val] Kappa: 0.8187 Accuracy: 0.6400 Precision: 0.6639 Recall: 0.6400\n",
      "\n",
      "Epoch 22/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.20 batch/s, lr=1.0e-06, Loss=0.5453]\n",
      "[Train] Kappa: 0.8820 Accuracy: 0.7158 Precision: 0.7127 Recall: 0.7158 Loss: 0.6871\n",
      "[Train] Class 0: Precision: 0.9171, Recall: 0.9833\n",
      "[Train] Class 1: Precision: 0.7004, Recall: 0.6917\n",
      "[Train] Class 2: Precision: 0.5702, Recall: 0.5417\n",
      "[Train] Class 3: Precision: 0.5929, Recall: 0.7708\n",
      "[Train] Class 4: Precision: 0.6486, Recall: 0.2000\n",
      "Evaluating: 100%|██████████| 17/17 [00:01<00:00,  8.69 batch/s]\n",
      "[Val] Kappa: 0.8205 Accuracy: 0.6400 Precision: 0.5651 Recall: 0.6400\n",
      "\n",
      "Epoch 23/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.30 batch/s, lr=1.0e-06, Loss=0.5926]\n",
      "[Train] Kappa: 0.8852 Accuracy: 0.7233 Precision: 0.7183 Recall: 0.7233 Loss: 0.6768\n",
      "[Train] Class 0: Precision: 0.9033, Recall: 0.9861\n",
      "[Train] Class 1: Precision: 0.7205, Recall: 0.6875\n",
      "[Train] Class 2: Precision: 0.6048, Recall: 0.5292\n",
      "[Train] Class 3: Precision: 0.5963, Recall: 0.8000\n",
      "[Train] Class 4: Precision: 0.6304, Recall: 0.2417\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.32 batch/s]\n",
      "[Val] Kappa: 0.8293 Accuracy: 0.6525 Precision: 0.6774 Recall: 0.6525\n",
      "\n",
      "Epoch 24/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.34 batch/s, lr=1.0e-06, Loss=0.7586]\n",
      "[Train] Kappa: 0.8850 Accuracy: 0.7325 Precision: 0.7344 Recall: 0.7325 Loss: 0.6756\n",
      "[Train] Class 0: Precision: 0.9152, Recall: 0.9889\n",
      "[Train] Class 1: Precision: 0.7521, Recall: 0.7333\n",
      "[Train] Class 2: Precision: 0.6049, Recall: 0.5167\n",
      "[Train] Class 3: Precision: 0.5848, Recall: 0.8042\n",
      "[Train] Class 4: Precision: 0.7143, Recall: 0.2500\n",
      "Evaluating: 100%|██████████| 17/17 [00:01<00:00,  8.61 batch/s]\n",
      "[Val] Kappa: 0.8008 Accuracy: 0.6350 Precision: 0.6581 Recall: 0.6350\n",
      "\n",
      "Epoch 25/25\n",
      "Training: 100%|██████████| 50/50 [00:11<00:00,  4.35 batch/s, lr=1.0e-06, Loss=0.6885]\n",
      "[Train] Kappa: 0.8855 Accuracy: 0.7350 Precision: 0.7298 Recall: 0.7350 Loss: 0.6615\n",
      "[Train] Class 0: Precision: 0.9191, Recall: 0.9778\n",
      "[Train] Class 1: Precision: 0.7542, Recall: 0.7417\n",
      "[Train] Class 2: Precision: 0.6091, Recall: 0.5583\n",
      "[Train] Class 3: Precision: 0.6032, Recall: 0.7792\n",
      "[Train] Class 4: Precision: 0.6078, Recall: 0.2583\n",
      "Evaluating: 100%|██████████| 17/17 [00:01<00:00,  9.24 batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/t87m92j14g9b1yw35n324r5w0000gn/T/ipykernel_30123/4198497785.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/res_a/a_2_extra_layer.pth', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Val] Kappa: 0.8152 Accuracy: 0.6525 Precision: 0.6762 Recall: 0.6525\n",
      "[Val] Best kappa: 0.8293, Epoch 23\n",
      "Evaluating: 100%|██████████| 17/17 [00:02<00:00,  8.34 batch/s]\n",
      "[Test] Save predictions to /Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/res_a/a_2_extra_layer.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Choose between 'single image' and 'dual images' pipeline\n",
    "    # This will affect the model definition, dataset pipeline, training and evaluation\n",
    "\n",
    "    mode = 'single'  # forward single image to the model each time\n",
    "    # mode = 'dual'  # forward two images of the same eye to the model and fuse the features\n",
    "\n",
    "    assert mode in ('single', 'dual')\n",
    "\n",
    "    # Define the model\n",
    "    if mode == 'single':\n",
    "        model = MyModel()\n",
    "    else:\n",
    "        model = MyDualModel()\n",
    "\n",
    "    print(model, '\\n')\n",
    "    print('Pipeline Mode:', mode)\n",
    "\n",
    "    # Create datasets\n",
    "\n",
    "    train_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/train/\"\n",
    "    train_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/train.csv\"\n",
    "    val_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/val/\"\n",
    "    val_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/val.csv\"\n",
    "    test_dir = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/test\"\n",
    "    test_ann_file = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/DeepDRiD/test.csv\"\n",
    "\n",
    "    train_dataset = RetinopathyDataset(ann_file=train_ann_file, image_dir=train_dir, mode=mode, transform=transform_train)\n",
    "    val_dataset = RetinopathyDataset(ann_file=val_ann_file, image_dir=val_dir, mode=mode, transform=transform_test)\n",
    "    test_dataset = RetinopathyDataset(ann_file=test_ann_file, image_dir=test_dir, mode=mode, transform=transform_test, test=True)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the weighted CrossEntropyLoss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Use GPU device is possible\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print('Device:', device)\n",
    "\n",
    "    # Move class weights to the device\n",
    "    #state_dict_path = \"/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/models/train_with_APTOS.pth\"\n",
    "    #state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "    #model.load_state_dict(state_dict, strict=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "\n",
    "    # Optimizer and Learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # Train and evaluate the model with the training and validation set\n",
    "    model = train_model(\n",
    "        model, train_loader, val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "        checkpoint_path='/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/res_a/a_2_extra_layer.pth'\n",
    "    )\n",
    "\n",
    "    # Load the pretrained checkpoint\n",
    "    state_dict = torch.load('/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/res_a/a_2_extra_layer.pth', map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Make predictions on testing set and save the prediction results\n",
    "    pred_path = '/Users/shakibibnashameem/Documents/Practice/deep_learning_fall_2024/final_project/data/results/res_a/a_2_extra_layer.csv'\n",
    "    evaluate_model(model, test_loader, device, test_only=True, prediction_path=pred_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
